# B∆∞·ªõc 4: Hu·∫•n luy·ªán m√¥ h√¨nh (Model Training)
import numpy as np
import time
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score
import pickle

class ModelTrainer:
    """
    L·ªõp th·ª±c hi·ªán hu·∫•n luy·ªán m√¥ h√¨nh machine learning
    B∆∞·ªõc 4 trong quy tr√¨nh x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n
    """
    
    def __init__(self):
        self.models = {}
        self.results = {}
        self.training_times = {}
    
    def prepare_data(self, X, y, test_size=0.2, random_state=42):
        """
        Chia d·ªØ li·ªáu th√†nh t·∫≠p train v√† test
        """
        try:
            print(f"Chia d·ªØ li·ªáu: {X.shape[0]} m·∫´u")
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, 
                test_size=test_size, 
                random_state=random_state, 
                stratify=y  # ƒê·∫£m b·∫£o t·ª∑ l·ªá nh√£n ƒë·ªÅu
            )
            
            print(f"Train set: {X_train.shape[0]} m·∫´u")
            print(f"Test set: {X_test.shape[0]} m·∫´u")
            print(f"Train spam ratio: {y_train.sum() / len(y_train):.3f}")
            print(f"Test spam ratio: {y_test.sum() / len(y_test):.3f}")
            
            return X_train, X_test, y_train, y_test
            
        except Exception as e:
            print(f"L·ªói chia d·ªØ li·ªáu: {e}")
            raise e
    
    def select_model(self, method_name, X_shape):
        """
        Ch·ªçn m√¥ h√¨nh ph√π h·ª£p v·ªõi lo·∫°i d·ªØ li·ªáu
        """
        if method_name == 'Sentence Embeddings':
            # GaussianNB cho continuous features (embeddings)
            model = GaussianNB()
        else:
            # MultinomialNB cho discrete features (BoW, TF-IDF)
            model = MultinomialNB(alpha=1.0)  # Laplace smoothing
        
        print(f"Ch·ªçn m√¥ h√¨nh: {type(model).__name__} cho {method_name}")
        return model
    
    def train_single_model(self, X, y, method_name, socket_callback=None):
        """
        Hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh ƒë∆°n l·∫ª
        """
        start_time = time.time()
        try:
            if socket_callback:
                socket_callback('training_progress', {
                    'method': method_name,
                    'status': 'splitting_data',
                    'message': f'Chia d·ªØ li·ªáu cho {method_name}...'
                })
            # Chia d·ªØ li·ªáu
            X_train, X_test, y_train, y_test = self.prepare_data(X, y)
            if socket_callback:
                socket_callback('training_progress', {
                    'method': method_name,
                    'status': 'training',
                    'message': f'Hu·∫•n luy·ªán m√¥ h√¨nh {method_name}...'
                })
            # Ch·ªçn v√† hu·∫•n luy·ªán m√¥ h√¨nh
            model = self.select_model(method_name, X.shape)
            model.fit(X_train, y_train)
            
            if socket_callback:
                socket_callback('training_progress', {
                    'method': method_name,
                    'status': 'evaluating',
                    'message': f'ƒê√°nh gi√° m√¥ h√¨nh {method_name}...'
                })
            # D·ª± ƒëo√°n v√† ƒë√°nh gi√°
            y_pred = model.predict(X_test)
            # T√≠nh c√°c metrics
            accuracy = accuracy_score(y_test, y_pred)
            # output_dict=True ƒë·ªÉ l·∫•y chi ti·∫øt macro avg
            report = classification_report(y_test, y_pred, output_dict=True) 
            cm = confusion_matrix(y_test, y_pred)
            training_time = time.time() - start_time
            # Cross-validation ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô ·ªïn ƒë·ªãnh
            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')
            result = {
                'model': model,
                'accuracy': accuracy,
                'classification_report': report,
                'confusion_matrix': cm,
                'y_test': y_test,
                'y_pred': y_pred,
                'training_time': training_time,
                'X_train': X_train,
                'y_train': y_train,
                'X_test': X_test,
                'cv_scores': cv_scores,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            # L∆∞u tr·ªØ k·∫øt qu·∫£
            self.models[method_name.lower().replace(' ', '_')] = model
            self.results[method_name.lower().replace(' ', '_')] = result
            self.training_times[method_name.lower().replace(' ', '_')] = training_time
            
            # S·ª¨A ƒê·ªîI: S·ª≠ d·ª•ng 'macro avg' thay cho 'weighted avg' trong log
            print(f"‚úÖ {method_name} - Accuracy: {accuracy:.4f}, Macro F1: {report['macro avg']['f1-score']:.4f}")
            
            return result
            
        except Exception as e:
            print(f"‚ùå L·ªói hu·∫•n luy·ªán {method_name}: {e}")
            raise e
    
    def train_all_models(self, features_dict, labels, socket_callback=None):
        """
        Hu·∫•n luy·ªán t·∫•t c·∫£ m√¥ h√¨nh v·ªõi c√°c lo·∫°i features kh√°c nhau
        """
        all_results = {}
        
        for method_name, features in features_dict.items():
            try:
                print(f"\nüöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán {method_name}...")
                result = self.train_single_model(features, labels, method_name, socket_callback)
                
                # Chu·∫©n b·ªã k·∫øt qu·∫£ cho frontend
                method_key = method_name.lower().replace(' ', '_')
                all_results[method_key] = {
                    'name': method_name,
                    'accuracy': float(result['accuracy']),
                    # S·ª¨A ƒê·ªîI: S·ª≠ d·ª•ng 'macro avg'
                    'precision': float(result['classification_report']['macro avg']['precision']),
                    'recall': float(result['classification_report']['macro avg']['recall']),
                    'f1_score': float(result['classification_report']['macro avg']['f1-score']),
                    'training_time': result['training_time'],
                    'cv_mean': float(result['cv_mean']),
                    'cv_std': float(result['cv_std'])
                }
                
                if socket_callback:
                    socket_callback('model_completed', {
                        'method': method_key,
                        'results': all_results[method_key]
                    })
                    
            except Exception as e:
                print(f"‚ùå {method_name} th·∫•t b·∫°i: {e}")
                if socket_callback:
                    socket_callback('model_error', {
                        'method': method_name,
                        'error': str(e)
                    })
                continue
        
        return all_results
    
    def get_best_model(self, metric='f1_score'):
        """
        T√¨m m√¥ h√¨nh t·ªët nh·∫•t d·ª±a tr√™n metric
        """
        if not self.results:
            return None
        
        best_method = None
        best_score = -1
        
        for method, result in self.results.items():
            # S·ª¨A ƒê·ªîI: S·ª≠ d·ª•ng 'macro avg' cho c√°c metrics
            if metric == 'f1_score':
                score = result['classification_report']['macro avg']['f1-score']
            elif metric == 'accuracy':
                score = result['accuracy']
            elif metric == 'precision':
                score = result['classification_report']['macro avg']['precision']
            elif metric == 'recall':
                score = result['classification_report']['macro avg']['recall']
            else:
                score = result['accuracy']
            
            if score > best_score:
                best_score = score
                best_method = method
        
        return best_method, best_score
    
    def predict_new_data(self, X_new, method='best'):
        """
        D·ª± ƒëo√°n d·ªØ li·ªáu m·ªõi
        """
        if method == 'best':
            method, _ = self.get_best_model()
        
        if method not in self.models:
            raise ValueError(f"M√¥ h√¨nh {method} kh√¥ng t·ªìn t·∫°i")
        
        model = self.models[method]
        predictions = model.predict(X_new)
        probabilities = model.predict_proba(X_new) if hasattr(model, 'predict_proba') else None
        
        return predictions, probabilities
    
    def save_models(self, filepath):
        """
        L∆∞u t·∫•t c·∫£ m√¥ h√¨nh v√†o file
        """
        try:
            model_package = {
                'models': self.models,
                'results': self.results,
                'training_times': self.training_times
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(model_package, f)
            
            print(f"ƒê√£ l∆∞u m√¥ h√¨nh v√†o {filepath}")
            
        except Exception as e:
            print(f"L·ªói l∆∞u m√¥ h√¨nh: {e}")
    
    def load_models(self, filepath):
        """
        T·∫£i m√¥ h√¨nh t·ª´ file
        """
        try:
            with open(filepath, 'rb') as f:
                model_package = pickle.load(f)
            
            self.models = model_package['models']
            self.results = model_package['results']
            self.training_times = model_package['training_times']
            
            print(f"ƒê√£ t·∫£i m√¥ h√¨nh t·ª´ {filepath}")
            
        except Exception as e:
            print(f"L·ªói t·∫£i m√¥ h√¨nh: {e}")
    
    def get_training_summary(self):
        """
        T√≥m t·∫Øt qu√° tr√¨nh hu·∫•n luy·ªán
        """
        if not self.results:
            return None
        
        summary = {
            'total_models': len(self.results),
            'total_training_time': sum(self.training_times.values()),
            'models': {}
        }
        
        for method, result in self.results.items():
            summary['models'][method] = {
                'accuracy': result['accuracy'],
                # S·ª¨A ƒê·ªîI: S·ª≠ d·ª•ng 'macro avg'
                'f1_score': result['classification_report']['macro avg']['f1-score'],
                'training_time': self.training_times[method],
                'cv_mean': result['cv_mean'],
                'cv_std': result['cv_std']
            }
        
        return summary